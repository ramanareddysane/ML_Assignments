{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reviews_sst = pd.read_csv('preprocessed_reviews_SST.csv')\n",
    "reviews_sst[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_sst['smry_txt'] = reviews_sst['Summary'].astype(str) + ' ' + reviews_sst['Text']\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets filter the dataset so that it contains reviews that are either positive(4 or 5) or negative(1 or 2)\n",
    "dataset = dataset[dataset.score != 3]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['score'] = dataset['score'].apply({1:'negative', 2:'negative', 4:'positive', 5:'positive'}.get)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the training(70%) and testing(30%) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['smry_txt'], dataset['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert our text into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ It basically convert our each text(smry_txt in this case) into a vector that represents the word and its number of  occurences, if it is present in that text/smry_txt and 0 otherwise.\n",
    "\n",
    "\n",
    "+ Even if you pass the raw text(Capital letters, special characters...), it will do all the preprocessing for you like removing stopwords, tokenizing etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on How Countvectorizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our count vectorizer learns about all the vocabulary of our data \n",
    "sample_count_vector = cv.fit([\"Hi How are you How are you doing\",\"Hi what's up\",\"Wow that's awesome\"])\n",
    "print('parameters for cv: \\n {}'.format(sample_count_vector.get_params()))\n",
    "print()\n",
    "print('features of our data : \\n {}'.format(sample_count_vector.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ ___Now we have a contVector that has all the features.___\n",
    "\n",
    "\n",
    "+ ___Let's just convert each of our data into a vector (bag of words model)___\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_x_data = sample_count_vector.transform([\"Hi How are you How are you doing\",\"Hi what's up\",\"Wow that's awesome\"])\n",
    "print(type(sample_x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We got a compressed sparse row matrix representation of the data.\n",
    "* Let's see the original feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_count_vector.get_feature_names() ,sample_x_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We can fit and transform the data at the same time.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer()\n",
    "#  It will learn about the data(ie., fit) that is given , and convert each text into a feature vector\n",
    "# and will return the vectors in Compressed Sparse Row format\n",
    "data_csr = cv1.fit_transform([\"Hi How are you How are you doing\",\"Hi what's up\",\"Wow that's awesome\"])\n",
    "print(cv1.get_feature_names())\n",
    "print( data_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets do  it with our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_cv = count_vect.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is how our each review stored in the vector\n",
    "print(x_train.iloc[0]+\"\\n\")\n",
    "print(count_vect.inverse_transform(x_train_cv[0]))\n",
    "print()\n",
    "print(x_train_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ###  let's find the best C value for logistic regression (best hyper parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   GridsearchCV will build classifier specified (logistic regression),\n",
    "# with the optimal C value, with 3-fold cross validation by defalut. \n",
    "# So, we don't need to build a classifier again with this model..\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "log_reg = LogisticRegression(penalty='l2')\n",
    "log_reg_grid_clf = GridSearchCV(log_reg, param_grid=param_grid,n_jobs=2, scoring='f1_micro')\n",
    "log_reg_grid_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vec_total = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = count_vec_total.fit_transform(dataset.smry_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = dataset['score']\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataset.iloc[0]['smry_txt'], end='\\n\\n')\n",
    "print(count_vec_total.inverse_transform(x_data[0]), end='\\n\\n')\n",
    "print(x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets run GridsearchCV on our data\n",
    "log_reg_grid_clf.fit(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
